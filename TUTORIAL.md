# metaTOR tutorial

This tutorial aims at providing users with quick commands to get the hang of the metaTOR pipeline, whether they want to reproduce steps described in [our papers and preprints](https://github.com/koszullab/metaTOR/blob/master/README.md#references) or adapt the commands to their own data.

This is *not* meant to provide a comprehensive explanation of all parameters and possible configurations to run the pipeline - please refer to the [metaTOR manual](https://github.com/koszullab/metaTOR/blob/master/meta3c_manual.pdf) if you want to delve into details. Rather, it is meant to be a step-by-step explanation of what you should be expected to run, tweak and visualize for the most common use cases.

## Getting the pipeline

Make sure you get the latest version of metaTOR:

```sh
   pip3 install -e git+https://github.com/koszullab/metator.git@master#egg=metator
```

## Installing dependencies

MetaTOR relies heavily on third-party tools such as ```samtools```, ```prodigal``` or ```bowtie2``` as well as some Python libraries. Because these are usually present in most OS's package managers, they are not included and can be simply downloaded, e.g. on Ubuntu 14.04+:

```bash
sudo apt install bowtie2 samtools hmmer prodigal
```

The pipeline also relies on external dependencies such as the [original Louvain algorithm implementation](https://sourceforge.net/projects/louvain/files/louvain-generic.tar.gz) as well as some [hidden Markov Model databases](http://dl.pasteur.fr/fop/5eHgTGww/modele_HMM.tar.gz) that aren't present in package managers. However, there's no need to download them by hand if you're not going to customize them, as you can simply run:

```bash
metator dependencies
```

which does the job for you.

## Preparing datasets

The very first inputs processed by metaTOR are 3C paired-end libraries in fastq format. Make sure they are properly cleaned, clipped and quality-filtered.

You also need a preliminary assembly to map those reads onto. It is usually
generated by running a assembler specialized in metagenomics on the reads themselves, e.g.
[MEGAHIT](https://github.com/voutcn/megahit) or
[metaSPAdes](https://github.com/ablab/spades/).

## Alignment

Run the following:

```bash
metator align -1 forward.fastq -2 reverse.fastq -a assembly.fa -C 100000000 -Q 10 --clean-up -p example_project
```

This can take a while and may fail abruptly if you don't have enough disk
space, so plan accordingly. We used ```bowtie2``` to align the reads, but you
can use ```minimap2``` if you have it and speed is a concern by adding the
```-m``` option. Tweak the ```-C``` and ```-Q``` parameters to adjust the chunk
size and minimum mapping quality, respectively. Note the above chunk size value is
extremely large, which is basically equivalent to *no* chunking. You may also use the ```-t``` option to run
it on several threads.

## Partition

Run the following:

```bash
metator partition --iterations 300
```

The number of iterations can be adjusted depending on time constraints. By default, all parameters you've set before are preserved from one step to the other, so you don't need to specify anything more. If you want to change something, simply set the option again and it will erase the previous value. If you want to reset everything, simply use the ```--reset``` option. If you want to check whether 300 iterations are sufficient, the pipeline helpfully plots the evolution of the size of core communities. Any stabilization indicates that more iterations are superfluous.

## Annotation

This step is independent from the other three, so you may run it in parallel on another environment where ```prodigal``` and ```hmmer``` are installed. If you do, make sure you copy and paste ```config.sh``` from the other instance of metaTOR to this one, or you will have to set the assembly ```-a``` and output folder ```-o``` options again. Otherwise, simply run:

```bash
metator annotation
```

Again, depending on the size of the assembly, the prediction software may take a while.

## Binning

The final step, merging annotations with core communities. Run:

```bash
metator binning --n-bins 100
```

This bins the 100 biggest cores only. You may bin all of them, though that's not recommended as many of them are singleton sequences that aren't very informative.

Inspect the ```output/partition``` folder and look at the figures. If you've used our publicly available datasets, you should note the visible bump in virus orthologous groups (VOGs) among smaller bins, as described in [Marbouty et al., 2017](http://advances.sciencemag.org/content/3/2/e1602105).

## Validating bins

You may want to make sure your bins are relatively complete and uncontaminated. We use checkM for this purpose, it can be easily installed:

```bash
sudo pip install checkm-genome
```

and used this way:

```bash
# Default directory if you've run the pipeline with the examples above
fasta_dir="output/example_project/partition/iteration300/fasta_merged"
checkm_dir="checkm_validation"

mkdir -p $checkm_dir

checkm tree -x fa $fasta_dir $checkm_dir # Add -t 8 for multithreading etc.
checkm tree_qa $checkm_dir -o 2 -f $checkm_dir/checkM_results.txt
checkm lineage_set $checkm_dir $checkm_dir/checkM_output_marker.txt
checkm analyze -x fa $checkm_dir/checkM_output_marker.txt $fasta_dir $checkm_dir
checkm qa -t 8 $checkm_dir/checkM_output_marker.txt ${checkm_dir} -o 2 > $checkm_dir/checkM_results_complete.txt &
```

This step is not included in the pipeline because it is so memory-hungry and not always necessary, depending on the use case.

## Recursive binning

If you have very contaminated bins that also happen to be very complete, this indicates that there are probably several genomes interleaved in the same bins. This can be simply solved by running the partition step again on the subnetworks of the bins:

```bash
subnetwork_dir="output/example_project/partition/iteration300/subnetworks"
./meta3c.sh partition --network-file $subnetwork_dir/contaminated_bin_subnetwork.dat --partition-dir subnetwork_partition_folder
./meta3c.sh binning
```